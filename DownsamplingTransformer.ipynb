{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7d297cc",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import math\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn # Contains all the functions we need to to train our network\n",
    "import torch.nn.functional as F # Contains some additional functions such as activations\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b9e5a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positional_embeddings(sequence_length, d): #\"i\" in paper corresponds to j - i.e. along dimension of size d\n",
    "    result = torch.ones(sequence_length, d) #pos in paper refers to which token - i.e. varying from 1 to 50\n",
    "    for i in range(sequence_length):\n",
    "      for j in range(d):\n",
    "        if j%2==0:\n",
    "          result[i,j] = math.sin(i/10000**(j/d))\n",
    "        else:\n",
    "          result[i,j] = math.cos(i/10000**((j-1)/d))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03cdde16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_d, out_d, n_heads, n_blocks):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        #self.class_token = nn.Parameter(torch.rand((input_dim)))\n",
    "        \n",
    "        self.linear = nn.Linear(input_dim, hidden_d)\n",
    "        \n",
    "        self.blocks = nn.ModuleList([MyViTBlock(hidden_d, n_heads) for _ in range(n_blocks)])\n",
    "        \n",
    "        self.mlp = nn.Linear(hidden_d,out_d)#for CELoss\n",
    "        #nn.Sequential(nn.Linear(hidden_d,out_d), nn.Softmax(dim=-1))\n",
    "        \n",
    "    def forward(self, sentence):\n",
    "        #print(sentence.shape)\n",
    "        pos = torch.unsqueeze(get_positional_embeddings(sentence.shape[-2], sentence.shape[-1]).to(device),0)\n",
    "        #out = torch.cat((torch.unsqueeze(self.class_token+token_pos,0),sentence[0]),1)\n",
    "        #print(sentence.shape)\n",
    "        out = sentence[0]\n",
    "        out = out+pos\n",
    "        out = self.linear(out) #so input is now hidden dim shape, i.e.1,s+1, h_d\n",
    "        for block in self.blocks:\n",
    "            out = block(out)    \n",
    "        out = torch.mean(out,dim=1)\n",
    "        \n",
    "        return torch.unsqueeze(self.mlp(out),0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e0306f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMSA(nn.Module):\n",
    "    \"\"\"MSA block\"\"\"\n",
    "    def __init__(self, d, n_heads=2):#d is hidden dim\n",
    "        super(MyMSA, self).__init__()\n",
    "        self.d = d\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        assert d % n_heads == 0, f\"Can't divide dimension {d} into {n_heads} heads\"\n",
    "\n",
    "        d_head = int(d / n_heads) #dim of each head\n",
    "        self.q_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n",
    "        self.k_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n",
    "        self.v_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n",
    "        self.d_head = d_head\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, sequences):\n",
    "        # Sequences has shape (N, seq_length, token_dim)\n",
    "        # We go into shape    (N, seq_length, n_heads, token_dim / n_heads)\n",
    "        # And come back to    (N, seq_length, item_dim)  (through concatenation)\n",
    "        result = []\n",
    "        for sequence in sequences:\n",
    "            seq_result = []\n",
    "            for head in range(self.n_heads):\n",
    "                q_mapping = self.q_mappings[head]\n",
    "                k_mapping = self.k_mappings[head]\n",
    "                v_mapping = self.v_mappings[head]\n",
    "                \n",
    "                #print(f\"{head=}\")\n",
    "\n",
    "                seq = sequence[:, head * self.d_head: (head + 1) * self.d_head] #interesting? so each attention head only looks at a subset of features\n",
    "                #print(seq.shape)\n",
    "                q, k, v = q_mapping(seq), k_mapping(seq), v_mapping(seq)\n",
    "\n",
    "                attention = self.softmax(q @ k.T / math.sqrt(self.d)) # here we take dot product between q and k vectors\n",
    "                seq_result.append(attention @ v) #and here we do a weighted sum over v vectors based on attentions\n",
    "            result.append(torch.hstack(seq_result))\n",
    "        return torch.cat([torch.unsqueeze(r, dim=0) for r in result])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b71b340",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyViTBlock(nn.Module):\n",
    "    def __init__(self, hidden_d, n_heads, mlp_ratio=4):\n",
    "        super(MyViTBlock, self).__init__()\n",
    "        self.hidden_d = hidden_d\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(hidden_d)\n",
    "        self.mhsa = MyMSA(hidden_d, n_heads)\n",
    "        self.norm2 = nn.LayerNorm(hidden_d)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_d, mlp_ratio * hidden_d),#i.e. mlp ratio tells us how much bigger mlp hidden is than previous hidden\n",
    "            nn.GELU(),\n",
    "            nn.Linear(mlp_ratio * hidden_d, hidden_d)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x + self.mhsa(self.norm1(x)) #so we do residual on multi self attention\n",
    "        out = out + self.mlp(self.norm2(out)) #then residual on mlp\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07f6a344",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownsamplingTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_d, out_d, n_heads, n_blocks):\n",
    "        super(DownsamplingTransformer, self).__init__()\n",
    "        input_dim, hidden_d, out_d, n_heads, n_blocks\n",
    "    def __init__(self, input_dim, hidden_d, out_d, n_heads, n_blocks):\n",
    "        super(tiered_transformer, self).__init__()\n",
    "        self.layer1 = Transformer(input_dim, hidden_d, hidden_d, n_heads, n_blocks)\n",
    "        self.layer2 = Transformer(hidden_d, hidden_d, out_d, n_heads, n_blocks)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #input is 1xNxpaddedx768\n",
    "        #transformers want 4 dim input, and do pos embedding themselves\n",
    "        \n",
    "        list_len = sentences.shape[1]\n",
    "        dim = sentences.shape[-1]\n",
    "        lengths = torch.sum((sentences[0]!=0),dim=1)[:,0]#get lengths of each sentence\n",
    "        lst = []\n",
    "        for i,elem in enumerate(sentences[0]):\n",
    "            elem = elem[0:lengths[i]]\n",
    "            fw = self.layer1(elem)\n",
    "            lst.append(fw)    \n",
    "\n",
    "        fws = torch.cat(lst)\n",
    "        \n",
    "        x = self.layer2(fws)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed74f45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

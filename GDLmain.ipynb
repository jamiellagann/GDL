{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4577c54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "pre_tokenizer = Whitespace()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0b4f6f",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import math\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn # Contains all the functions we need to to train our network\n",
    "import torch.nn.functional as F # Contains some additional functions such as activations\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ed8b3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "model_name = \"bert-base-uncased\"\n",
    "model = BertModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4a4c326",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c293577",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"C:/Users/______/Desktop/GDL project/yelp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0894675",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "928b6c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 10)\n",
      "Index(['business_id', 'date', 'review_id', 'stars', 'text', 'type', 'user_id',\n",
      "       'cool', 'useful', 'funny'],\n",
      "      dtype='object')\n",
      "              business_id        date               review_id  stars   \n",
      "0  9yKzy9PApeiPPOUJEtnvkg  2011-01-26  fWKvX83p0-ka4JS3dc6E5A      5  \\\n",
      "1  ZRJwVLyzEJq1VAihDhYiow  2011-07-27  IjZ33sJrzXqU-0X6U8NwyA      5   \n",
      "2  6oRAC4uyJCsJl1X0WZpVSA  2012-06-14  IESLBzqUCLdSzSqm0eCSxQ      4   \n",
      "3  _1QQZuf4zZOyFCvXc0o6Vg  2010-05-27  G-WvGaISbqqaMHlNnByodA      5   \n",
      "4  6ozycU1RpktNG2-1BroVtw  2012-01-05  1uJFq2r5QfJG_6ExMRCaGw      5   \n",
      "\n",
      "                                                text    type   \n",
      "0  My wife took me here on my birthday for breakf...  review  \\\n",
      "1  I have no idea why some people give bad review...  review   \n",
      "2  love the gyro plate. Rice is so good and I als...  review   \n",
      "3  Rosie, Dakota, and I LOVE Chaparral Dog Park!!...  review   \n",
      "4  General Manager Scott Petello is a good egg!!!...  review   \n",
      "\n",
      "                  user_id  cool  useful  funny  \n",
      "0  rLtl8ZkDX5vH5nAx9C3q5Q     2       5      0  \n",
      "1  0a2KyEL0d3Yb1V6aivbIuQ     0       0      0  \n",
      "2  0hT2KtfLiobPvh6cDC8JQg     0       1      0  \n",
      "3  uZetl9T0NcROGOyFfughhg     1       2      0  \n",
      "4  vYmM4KTsC8ZfQBg-j5MWkw     0       0      0  \n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "\n",
    "print(data.columns)\n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "384ff7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(strr):\n",
    "    words = strr.replace(\".\", \" \")\n",
    "    word_list = words.split(\" \")\n",
    "    return len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d63f1c23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9yKzy9PApeiPPOUJEtnvkg</td>\n",
       "      <td>2011-01-26</td>\n",
       "      <td>fWKvX83p0-ka4JS3dc6E5A</td>\n",
       "      <td>5</td>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "      <td>review</td>\n",
       "      <td>rLtl8ZkDX5vH5nAx9C3q5Q</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZRJwVLyzEJq1VAihDhYiow</td>\n",
       "      <td>2011-07-27</td>\n",
       "      <td>IjZ33sJrzXqU-0X6U8NwyA</td>\n",
       "      <td>5</td>\n",
       "      <td>I have no idea why some people give bad review...</td>\n",
       "      <td>review</td>\n",
       "      <td>0a2KyEL0d3Yb1V6aivbIuQ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6oRAC4uyJCsJl1X0WZpVSA</td>\n",
       "      <td>2012-06-14</td>\n",
       "      <td>IESLBzqUCLdSzSqm0eCSxQ</td>\n",
       "      <td>4</td>\n",
       "      <td>love the gyro plate. Rice is so good and I als...</td>\n",
       "      <td>review</td>\n",
       "      <td>0hT2KtfLiobPvh6cDC8JQg</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_1QQZuf4zZOyFCvXc0o6Vg</td>\n",
       "      <td>2010-05-27</td>\n",
       "      <td>G-WvGaISbqqaMHlNnByodA</td>\n",
       "      <td>5</td>\n",
       "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>uZetl9T0NcROGOyFfughhg</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6ozycU1RpktNG2-1BroVtw</td>\n",
       "      <td>2012-01-05</td>\n",
       "      <td>1uJFq2r5QfJG_6ExMRCaGw</td>\n",
       "      <td>5</td>\n",
       "      <td>General Manager Scott Petello is a good egg!!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>vYmM4KTsC8ZfQBg-j5MWkw</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id        date               review_id  stars   \n",
       "0  9yKzy9PApeiPPOUJEtnvkg  2011-01-26  fWKvX83p0-ka4JS3dc6E5A      5  \\\n",
       "1  ZRJwVLyzEJq1VAihDhYiow  2011-07-27  IjZ33sJrzXqU-0X6U8NwyA      5   \n",
       "2  6oRAC4uyJCsJl1X0WZpVSA  2012-06-14  IESLBzqUCLdSzSqm0eCSxQ      4   \n",
       "3  _1QQZuf4zZOyFCvXc0o6Vg  2010-05-27  G-WvGaISbqqaMHlNnByodA      5   \n",
       "4  6ozycU1RpktNG2-1BroVtw  2012-01-05  1uJFq2r5QfJG_6ExMRCaGw      5   \n",
       "\n",
       "                                                text    type   \n",
       "0  My wife took me here on my birthday for breakf...  review  \\\n",
       "1  I have no idea why some people give bad review...  review   \n",
       "2  love the gyro plate. Rice is so good and I als...  review   \n",
       "3  Rosie, Dakota, and I LOVE Chaparral Dog Park!!...  review   \n",
       "4  General Manager Scott Petello is a good egg!!!...  review   \n",
       "\n",
       "                  user_id  cool  useful  funny  length  \n",
       "0  rLtl8ZkDX5vH5nAx9C3q5Q     2       5      0     171  \n",
       "1  0a2KyEL0d3Yb1V6aivbIuQ     0       0      0     274  \n",
       "2  0hT2KtfLiobPvh6cDC8JQg     0       1      0      17  \n",
       "3  uZetl9T0NcROGOyFfughhg     1       2      0      79  \n",
       "4  vYmM4KTsC8ZfQBg-j5MWkw     0       0      0     102  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CREATING A NEW COLUMN IN THE DATASET FOR THE NUMBER OF WORDS IN THE REVIEW\n",
    "data['length'] = data['text'].apply(get_words)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6435bf3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>length</th>\n",
       "      <th>maxl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9yKzy9PApeiPPOUJEtnvkg</td>\n",
       "      <td>2011-01-26</td>\n",
       "      <td>fWKvX83p0-ka4JS3dc6E5A</td>\n",
       "      <td>5</td>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "      <td>review</td>\n",
       "      <td>rLtl8ZkDX5vH5nAx9C3q5Q</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>171</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZRJwVLyzEJq1VAihDhYiow</td>\n",
       "      <td>2011-07-27</td>\n",
       "      <td>IjZ33sJrzXqU-0X6U8NwyA</td>\n",
       "      <td>5</td>\n",
       "      <td>I have no idea why some people give bad review...</td>\n",
       "      <td>review</td>\n",
       "      <td>0a2KyEL0d3Yb1V6aivbIuQ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>274</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6oRAC4uyJCsJl1X0WZpVSA</td>\n",
       "      <td>2012-06-14</td>\n",
       "      <td>IESLBzqUCLdSzSqm0eCSxQ</td>\n",
       "      <td>4</td>\n",
       "      <td>love the gyro plate. Rice is so good and I als...</td>\n",
       "      <td>review</td>\n",
       "      <td>0hT2KtfLiobPvh6cDC8JQg</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_1QQZuf4zZOyFCvXc0o6Vg</td>\n",
       "      <td>2010-05-27</td>\n",
       "      <td>G-WvGaISbqqaMHlNnByodA</td>\n",
       "      <td>5</td>\n",
       "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>uZetl9T0NcROGOyFfughhg</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6ozycU1RpktNG2-1BroVtw</td>\n",
       "      <td>2012-01-05</td>\n",
       "      <td>1uJFq2r5QfJG_6ExMRCaGw</td>\n",
       "      <td>5</td>\n",
       "      <td>General Manager Scott Petello is a good egg!!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>vYmM4KTsC8ZfQBg-j5MWkw</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>102</td>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id        date               review_id  stars   \n",
       "0  9yKzy9PApeiPPOUJEtnvkg  2011-01-26  fWKvX83p0-ka4JS3dc6E5A      5  \\\n",
       "1  ZRJwVLyzEJq1VAihDhYiow  2011-07-27  IjZ33sJrzXqU-0X6U8NwyA      5   \n",
       "2  6oRAC4uyJCsJl1X0WZpVSA  2012-06-14  IESLBzqUCLdSzSqm0eCSxQ      4   \n",
       "3  _1QQZuf4zZOyFCvXc0o6Vg  2010-05-27  G-WvGaISbqqaMHlNnByodA      5   \n",
       "4  6ozycU1RpktNG2-1BroVtw  2012-01-05  1uJFq2r5QfJG_6ExMRCaGw      5   \n",
       "\n",
       "                                                text    type   \n",
       "0  My wife took me here on my birthday for breakf...  review  \\\n",
       "1  I have no idea why some people give bad review...  review   \n",
       "2  love the gyro plate. Rice is so good and I als...  review   \n",
       "3  Rosie, Dakota, and I LOVE Chaparral Dog Park!!...  review   \n",
       "4  General Manager Scott Petello is a good egg!!!...  review   \n",
       "\n",
       "                  user_id  cool  useful  funny  length  maxl  \n",
       "0  rLtl8ZkDX5vH5nAx9C3q5Q     2       5      0     171   135  \n",
       "1  0a2KyEL0d3Yb1V6aivbIuQ     0       0      0     274   201  \n",
       "2  0hT2KtfLiobPvh6cDC8JQg     0       1      0      17    56  \n",
       "3  uZetl9T0NcROGOyFfughhg     1       2      0      79   172  \n",
       "4  vYmM4KTsC8ZfQBg-j5MWkw     0       0      0     102   279  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"maxl\"] = data[\"text\"].apply(lambda x: max(map(len,x.split('.'))))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2370b0c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x1c223f2b2b0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x300 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# COMPARING TEXT LENGTH TO STARS\n",
    "graph = sns.FacetGrid(data=data,col='stars')\n",
    "graph.map(plt.hist,'length',bins=50,color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f46f2e98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>length</th>\n",
       "      <th>maxl</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stars</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.576769</td>\n",
       "      <td>1.604806</td>\n",
       "      <td>1.056075</td>\n",
       "      <td>167.941255</td>\n",
       "      <td>163.433912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.719525</td>\n",
       "      <td>1.563107</td>\n",
       "      <td>0.875944</td>\n",
       "      <td>170.721683</td>\n",
       "      <td>163.048544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.788501</td>\n",
       "      <td>1.306639</td>\n",
       "      <td>0.694730</td>\n",
       "      <td>154.157426</td>\n",
       "      <td>152.344285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.954623</td>\n",
       "      <td>1.395916</td>\n",
       "      <td>0.670448</td>\n",
       "      <td>142.790698</td>\n",
       "      <td>155.195406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.944261</td>\n",
       "      <td>1.381780</td>\n",
       "      <td>0.608631</td>\n",
       "      <td>124.482769</td>\n",
       "      <td>156.760264</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           cool    useful     funny      length        maxl\n",
       "stars                                                      \n",
       "1      0.576769  1.604806  1.056075  167.941255  163.433912\n",
       "2      0.719525  1.563107  0.875944  170.721683  163.048544\n",
       "3      0.788501  1.306639  0.694730  154.157426  152.344285\n",
       "4      0.954623  1.395916  0.670448  142.790698  155.195406\n",
       "5      0.944261  1.381780  0.608631  124.482769  156.760264"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GETTING THE MEAN VALUES OF THE VOTE COLUMNS WRT THE STARS ON THE REVIEW\n",
    "stval = data.groupby('stars')\n",
    "stval.mean(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5cd3aa15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>length</th>\n",
       "      <th>maxl</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stars</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>749</td>\n",
       "      <td>749</td>\n",
       "      <td>749</td>\n",
       "      <td>749</td>\n",
       "      <td>749</td>\n",
       "      <td>749</td>\n",
       "      <td>749</td>\n",
       "      <td>749</td>\n",
       "      <td>749</td>\n",
       "      <td>749</td>\n",
       "      <td>749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>927</td>\n",
       "      <td>927</td>\n",
       "      <td>927</td>\n",
       "      <td>927</td>\n",
       "      <td>927</td>\n",
       "      <td>927</td>\n",
       "      <td>927</td>\n",
       "      <td>927</td>\n",
       "      <td>927</td>\n",
       "      <td>927</td>\n",
       "      <td>927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1461</td>\n",
       "      <td>1461</td>\n",
       "      <td>1461</td>\n",
       "      <td>1461</td>\n",
       "      <td>1461</td>\n",
       "      <td>1461</td>\n",
       "      <td>1461</td>\n",
       "      <td>1461</td>\n",
       "      <td>1461</td>\n",
       "      <td>1461</td>\n",
       "      <td>1461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3526</td>\n",
       "      <td>3526</td>\n",
       "      <td>3526</td>\n",
       "      <td>3526</td>\n",
       "      <td>3526</td>\n",
       "      <td>3526</td>\n",
       "      <td>3526</td>\n",
       "      <td>3526</td>\n",
       "      <td>3526</td>\n",
       "      <td>3526</td>\n",
       "      <td>3526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3337</td>\n",
       "      <td>3337</td>\n",
       "      <td>3337</td>\n",
       "      <td>3337</td>\n",
       "      <td>3337</td>\n",
       "      <td>3337</td>\n",
       "      <td>3337</td>\n",
       "      <td>3337</td>\n",
       "      <td>3337</td>\n",
       "      <td>3337</td>\n",
       "      <td>3337</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       business_id  date  review_id  text  type  user_id  cool  useful  funny   \n",
       "stars                                                                           \n",
       "1              749   749        749   749   749      749   749     749    749  \\\n",
       "2              927   927        927   927   927      927   927     927    927   \n",
       "3             1461  1461       1461  1461  1461     1461  1461    1461   1461   \n",
       "4             3526  3526       3526  3526  3526     3526  3526    3526   3526   \n",
       "5             3337  3337       3337  3337  3337     3337  3337    3337   3337   \n",
       "\n",
       "       length  maxl  \n",
       "stars                \n",
       "1         749   749  \n",
       "2         927   927  \n",
       "3        1461  1461  \n",
       "4        3526  3526  \n",
       "5        3337  3337  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stval.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ac9bedb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>length</th>\n",
       "      <th>maxl</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stars</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th>cool</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.759015</td>\n",
       "      <td>0.802161</td>\n",
       "      <td>0.268091</td>\n",
       "      <td>0.194253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>useful</th>\n",
       "      <td>0.759015</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.667355</td>\n",
       "      <td>0.325898</td>\n",
       "      <td>0.236515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>funny</th>\n",
       "      <td>0.802161</td>\n",
       "      <td>0.667355</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.256482</td>\n",
       "      <td>0.170644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>length</th>\n",
       "      <td>0.268091</td>\n",
       "      <td>0.325898</td>\n",
       "      <td>0.256482</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.579887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>maxl</th>\n",
       "      <td>0.194253</td>\n",
       "      <td>0.236515</td>\n",
       "      <td>0.170644</td>\n",
       "      <td>0.579887</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2</th>\n",
       "      <th>cool</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.785922</td>\n",
       "      <td>0.826350</td>\n",
       "      <td>0.269454</td>\n",
       "      <td>0.266398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>useful</th>\n",
       "      <td>0.785922</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.669214</td>\n",
       "      <td>0.304540</td>\n",
       "      <td>0.232282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>funny</th>\n",
       "      <td>0.826350</td>\n",
       "      <td>0.669214</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.255107</td>\n",
       "      <td>0.283420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>length</th>\n",
       "      <td>0.269454</td>\n",
       "      <td>0.304540</td>\n",
       "      <td>0.255107</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.483595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>maxl</th>\n",
       "      <td>0.266398</td>\n",
       "      <td>0.232282</td>\n",
       "      <td>0.283420</td>\n",
       "      <td>0.483595</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">3</th>\n",
       "      <th>cool</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.853302</td>\n",
       "      <td>0.832051</td>\n",
       "      <td>0.252712</td>\n",
       "      <td>0.217422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>useful</th>\n",
       "      <td>0.853302</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.760241</td>\n",
       "      <td>0.290980</td>\n",
       "      <td>0.247937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>funny</th>\n",
       "      <td>0.832051</td>\n",
       "      <td>0.760241</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.252378</td>\n",
       "      <td>0.211076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>length</th>\n",
       "      <td>0.252712</td>\n",
       "      <td>0.290980</td>\n",
       "      <td>0.252378</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.528993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>maxl</th>\n",
       "      <td>0.217422</td>\n",
       "      <td>0.247937</td>\n",
       "      <td>0.211076</td>\n",
       "      <td>0.528993</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">4</th>\n",
       "      <th>cool</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.902031</td>\n",
       "      <td>0.851593</td>\n",
       "      <td>0.274769</td>\n",
       "      <td>0.202739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>useful</th>\n",
       "      <td>0.902031</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.794473</td>\n",
       "      <td>0.307464</td>\n",
       "      <td>0.208096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>funny</th>\n",
       "      <td>0.851593</td>\n",
       "      <td>0.794473</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.236706</td>\n",
       "      <td>0.170999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>length</th>\n",
       "      <td>0.274769</td>\n",
       "      <td>0.307464</td>\n",
       "      <td>0.236706</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.541772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>maxl</th>\n",
       "      <td>0.202739</td>\n",
       "      <td>0.208096</td>\n",
       "      <td>0.170999</td>\n",
       "      <td>0.541772</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">5</th>\n",
       "      <th>cool</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.926913</td>\n",
       "      <td>0.718411</td>\n",
       "      <td>0.235009</td>\n",
       "      <td>0.137575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>useful</th>\n",
       "      <td>0.926913</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.692654</td>\n",
       "      <td>0.275695</td>\n",
       "      <td>0.156358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>funny</th>\n",
       "      <td>0.718411</td>\n",
       "      <td>0.692654</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.249240</td>\n",
       "      <td>0.146321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>length</th>\n",
       "      <td>0.235009</td>\n",
       "      <td>0.275695</td>\n",
       "      <td>0.249240</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.496588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>maxl</th>\n",
       "      <td>0.137575</td>\n",
       "      <td>0.156358</td>\n",
       "      <td>0.146321</td>\n",
       "      <td>0.496588</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  cool    useful     funny    length      maxl\n",
       "stars                                                         \n",
       "1     cool    1.000000  0.759015  0.802161  0.268091  0.194253\n",
       "      useful  0.759015  1.000000  0.667355  0.325898  0.236515\n",
       "      funny   0.802161  0.667355  1.000000  0.256482  0.170644\n",
       "      length  0.268091  0.325898  0.256482  1.000000  0.579887\n",
       "      maxl    0.194253  0.236515  0.170644  0.579887  1.000000\n",
       "2     cool    1.000000  0.785922  0.826350  0.269454  0.266398\n",
       "      useful  0.785922  1.000000  0.669214  0.304540  0.232282\n",
       "      funny   0.826350  0.669214  1.000000  0.255107  0.283420\n",
       "      length  0.269454  0.304540  0.255107  1.000000  0.483595\n",
       "      maxl    0.266398  0.232282  0.283420  0.483595  1.000000\n",
       "3     cool    1.000000  0.853302  0.832051  0.252712  0.217422\n",
       "      useful  0.853302  1.000000  0.760241  0.290980  0.247937\n",
       "      funny   0.832051  0.760241  1.000000  0.252378  0.211076\n",
       "      length  0.252712  0.290980  0.252378  1.000000  0.528993\n",
       "      maxl    0.217422  0.247937  0.211076  0.528993  1.000000\n",
       "4     cool    1.000000  0.902031  0.851593  0.274769  0.202739\n",
       "      useful  0.902031  1.000000  0.794473  0.307464  0.208096\n",
       "      funny   0.851593  0.794473  1.000000  0.236706  0.170999\n",
       "      length  0.274769  0.307464  0.236706  1.000000  0.541772\n",
       "      maxl    0.202739  0.208096  0.170999  0.541772  1.000000\n",
       "5     cool    1.000000  0.926913  0.718411  0.235009  0.137575\n",
       "      useful  0.926913  1.000000  0.692654  0.275695  0.156358\n",
       "      funny   0.718411  0.692654  1.000000  0.249240  0.146321\n",
       "      length  0.235009  0.275695  0.249240  1.000000  0.496588\n",
       "      maxl    0.137575  0.156358  0.146321  0.496588  1.000000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stval.corr(numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "737b6a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5547, 12)\n"
     ]
    }
   ],
   "source": [
    "# CLASSIFICATION\n",
    "data_classes = data[(data['stars']==1) | (data['stars']==3) | (data['stars']==5)]\n",
    "data_classes.head()\n",
    "print(data_classes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4b0771ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_classes = data_classes[data_classes[\"maxl\"]<100]\n",
    "data_classes = data_classes[data_classes[\"length\"]<200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0169634f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>length</th>\n",
       "      <th>maxl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9yKzy9PApeiPPOUJEtnvkg</td>\n",
       "      <td>2011-01-26</td>\n",
       "      <td>fWKvX83p0-ka4JS3dc6E5A</td>\n",
       "      <td>5</td>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "      <td>review</td>\n",
       "      <td>rLtl8ZkDX5vH5nAx9C3q5Q</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>171</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_1QQZuf4zZOyFCvXc0o6Vg</td>\n",
       "      <td>2010-05-27</td>\n",
       "      <td>G-WvGaISbqqaMHlNnByodA</td>\n",
       "      <td>5</td>\n",
       "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>uZetl9T0NcROGOyFfughhg</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>nMHhuYan8e3cONo3PornJA</td>\n",
       "      <td>2010-08-11</td>\n",
       "      <td>jJAIXA46pU1swYyRCdfXtQ</td>\n",
       "      <td>5</td>\n",
       "      <td>Nobuo shows his unique talents with everything...</td>\n",
       "      <td>review</td>\n",
       "      <td>sUNkXg8-KFtCMQDV6zRzQg</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>AsSCv0q_BWqIe3mX2JqsOQ</td>\n",
       "      <td>2010-06-16</td>\n",
       "      <td>E11jzpKz9Kw5K7fuARWfRw</td>\n",
       "      <td>5</td>\n",
       "      <td>The oldish man who owns the store is as sweet ...</td>\n",
       "      <td>review</td>\n",
       "      <td>-OMlS6yWkYjVldNhC31wYg</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>59</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>e9nN4XxjdHj4qtKCOPq_vg</td>\n",
       "      <td>2011-10-21</td>\n",
       "      <td>3rPt0LxF7rgmEUrznoH22w</td>\n",
       "      <td>5</td>\n",
       "      <td>Wonderful Vietnamese sandwich shoppe. Their ba...</td>\n",
       "      <td>review</td>\n",
       "      <td>C1rHp3dmepNea7XiouwB6Q</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9990</th>\n",
       "      <td>R8VwdLyvsp9iybNqRvm94g</td>\n",
       "      <td>2011-10-03</td>\n",
       "      <td>pcEeHdAJPoFNF23es0kKWg</td>\n",
       "      <td>5</td>\n",
       "      <td>Yes I do rock the hipster joints.  I dig this ...</td>\n",
       "      <td>review</td>\n",
       "      <td>b92Y3tyWTQQZ5FLifex62Q</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>66</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9991</th>\n",
       "      <td>WJ5mq4EiWYAA4Vif0xDfdg</td>\n",
       "      <td>2011-12-05</td>\n",
       "      <td>EuHX-39FR7tyyG1ElvN1Jw</td>\n",
       "      <td>5</td>\n",
       "      <td>Only 4 stars? \\n\\n(A few notes: The folks that...</td>\n",
       "      <td>review</td>\n",
       "      <td>hTau-iNZFwoNsPCaiIUTEA</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>177</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9992</th>\n",
       "      <td>f96lWMIAUhYIYy9gOktivQ</td>\n",
       "      <td>2009-03-10</td>\n",
       "      <td>YF17z7HWlMj6aezZc-pVEw</td>\n",
       "      <td>5</td>\n",
       "      <td>I'm not normally one to jump at reviewing a ch...</td>\n",
       "      <td>review</td>\n",
       "      <td>W_QXYA7A0IhMrvbckz7eVg</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>280</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>VY_tvNUCCXGXQeSvJl757Q</td>\n",
       "      <td>2012-07-28</td>\n",
       "      <td>Ubyfp2RSDYW0g7Mbr8N3iA</td>\n",
       "      <td>3</td>\n",
       "      <td>First visit...Had lunch here today - used my G...</td>\n",
       "      <td>review</td>\n",
       "      <td>_eqQoPtQ3e3UxLE4faT6ow</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>132</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>pF7uRzygyZsltbmVpjIyvw</td>\n",
       "      <td>2010-10-16</td>\n",
       "      <td>vWSmOhg2ID1MNZHaWapGbA</td>\n",
       "      <td>5</td>\n",
       "      <td>4-5 locations.. all 4.5 star average.. I think...</td>\n",
       "      <td>review</td>\n",
       "      <td>KSBFytcdjPKZgXKQnYQdkA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>95</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4266 rows  12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 business_id        date               review_id  stars   \n",
       "0     9yKzy9PApeiPPOUJEtnvkg  2011-01-26  fWKvX83p0-ka4JS3dc6E5A      5  \\\n",
       "3     _1QQZuf4zZOyFCvXc0o6Vg  2010-05-27  G-WvGaISbqqaMHlNnByodA      5   \n",
       "9     nMHhuYan8e3cONo3PornJA  2010-08-11  jJAIXA46pU1swYyRCdfXtQ      5   \n",
       "10    AsSCv0q_BWqIe3mX2JqsOQ  2010-06-16  E11jzpKz9Kw5K7fuARWfRw      5   \n",
       "11    e9nN4XxjdHj4qtKCOPq_vg  2011-10-21  3rPt0LxF7rgmEUrznoH22w      5   \n",
       "...                      ...         ...                     ...    ...   \n",
       "9990  R8VwdLyvsp9iybNqRvm94g  2011-10-03  pcEeHdAJPoFNF23es0kKWg      5   \n",
       "9991  WJ5mq4EiWYAA4Vif0xDfdg  2011-12-05  EuHX-39FR7tyyG1ElvN1Jw      5   \n",
       "9992  f96lWMIAUhYIYy9gOktivQ  2009-03-10  YF17z7HWlMj6aezZc-pVEw      5   \n",
       "9995  VY_tvNUCCXGXQeSvJl757Q  2012-07-28  Ubyfp2RSDYW0g7Mbr8N3iA      3   \n",
       "9999  pF7uRzygyZsltbmVpjIyvw  2010-10-16  vWSmOhg2ID1MNZHaWapGbA      5   \n",
       "\n",
       "                                                   text    type   \n",
       "0     My wife took me here on my birthday for breakf...  review  \\\n",
       "3     Rosie, Dakota, and I LOVE Chaparral Dog Park!!...  review   \n",
       "9     Nobuo shows his unique talents with everything...  review   \n",
       "10    The oldish man who owns the store is as sweet ...  review   \n",
       "11    Wonderful Vietnamese sandwich shoppe. Their ba...  review   \n",
       "...                                                 ...     ...   \n",
       "9990  Yes I do rock the hipster joints.  I dig this ...  review   \n",
       "9991  Only 4 stars? \\n\\n(A few notes: The folks that...  review   \n",
       "9992  I'm not normally one to jump at reviewing a ch...  review   \n",
       "9995  First visit...Had lunch here today - used my G...  review   \n",
       "9999  4-5 locations.. all 4.5 star average.. I think...  review   \n",
       "\n",
       "                     user_id  cool  useful  funny  length  maxl  \n",
       "0     rLtl8ZkDX5vH5nAx9C3q5Q     2       5      0     171   135  \n",
       "3     uZetl9T0NcROGOyFfughhg     1       2      0      79   172  \n",
       "9     sUNkXg8-KFtCMQDV6zRzQg     0       1      0      38    58  \n",
       "10    -OMlS6yWkYjVldNhC31wYg     1       3      1      59    71  \n",
       "11    C1rHp3dmepNea7XiouwB6Q     1       1      0      63    99  \n",
       "...                      ...   ...     ...    ...     ...   ...  \n",
       "9990  b92Y3tyWTQQZ5FLifex62Q     1       1      1      66    92  \n",
       "9991  hTau-iNZFwoNsPCaiIUTEA     1       1      0     177   167  \n",
       "9992  W_QXYA7A0IhMrvbckz7eVg     2       3      2     280   142  \n",
       "9995  _eqQoPtQ3e3UxLE4faT6ow     1       2      0     132   115  \n",
       "9999  KSBFytcdjPKZgXKQnYQdkA     0       0      0      95   132  \n",
       "\n",
       "[4266 rows x 12 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d0e97ecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x1c21fcfab00>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x300 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "graph = sns.FacetGrid(data=data_classes,col='stars')\n",
    "graph.map(plt.hist,'length',bins=50,color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "044f0811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>length</th>\n",
       "      <th>maxl</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stars</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.441652</td>\n",
       "      <td>1.346499</td>\n",
       "      <td>0.827648</td>\n",
       "      <td>120.978456</td>\n",
       "      <td>125.267504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.607517</td>\n",
       "      <td>1.064685</td>\n",
       "      <td>0.510490</td>\n",
       "      <td>121.050699</td>\n",
       "      <td>121.980769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.796101</td>\n",
       "      <td>1.198830</td>\n",
       "      <td>0.498246</td>\n",
       "      <td>98.076803</td>\n",
       "      <td>118.132554</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           cool    useful     funny      length        maxl\n",
       "stars                                                      \n",
       "1      0.441652  1.346499  0.827648  120.978456  125.267504\n",
       "3      0.607517  1.064685  0.510490  121.050699  121.980769\n",
       "5      0.796101  1.198830  0.498246   98.076803  118.132554"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stval2 = data_classes.groupby('stars')\n",
    "stval2.mean(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ad409613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>length</th>\n",
       "      <th>maxl</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stars</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>557</td>\n",
       "      <td>557</td>\n",
       "      <td>557</td>\n",
       "      <td>557</td>\n",
       "      <td>557</td>\n",
       "      <td>557</td>\n",
       "      <td>557</td>\n",
       "      <td>557</td>\n",
       "      <td>557</td>\n",
       "      <td>557</td>\n",
       "      <td>557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1144</td>\n",
       "      <td>1144</td>\n",
       "      <td>1144</td>\n",
       "      <td>1144</td>\n",
       "      <td>1144</td>\n",
       "      <td>1144</td>\n",
       "      <td>1144</td>\n",
       "      <td>1144</td>\n",
       "      <td>1144</td>\n",
       "      <td>1144</td>\n",
       "      <td>1144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2565</td>\n",
       "      <td>2565</td>\n",
       "      <td>2565</td>\n",
       "      <td>2565</td>\n",
       "      <td>2565</td>\n",
       "      <td>2565</td>\n",
       "      <td>2565</td>\n",
       "      <td>2565</td>\n",
       "      <td>2565</td>\n",
       "      <td>2565</td>\n",
       "      <td>2565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       business_id  date  review_id  text  type  user_id  cool  useful  funny   \n",
       "stars                                                                           \n",
       "1              557   557        557   557   557      557   557     557    557  \\\n",
       "3             1144  1144       1144  1144  1144     1144  1144    1144   1144   \n",
       "5             2565  2565       2565  2565  2565     2565  2565    2565   2565   \n",
       "\n",
       "       length  maxl  \n",
       "stars                \n",
       "1         557   557  \n",
       "3        1144  1144  \n",
       "5        2565  2565  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stval2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48eff3af",
   "metadata": {},
   "source": [
    "train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e0af0ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.concat([data_classes[data_classes[\"stars\"]==i][0:333] for i in (1,3,5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9912aa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = data_train.groupby(\"length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ba0393b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>maxl</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>length</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>278 rows  11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        business_id  date  review_id  stars  text  type  user_id  cool   \n",
       "length                                                                   \n",
       "2                 5     5          5      5     5     5        5     5  \\\n",
       "3                 4     4          4      4     4     4        4     4   \n",
       "6                 4     4          4      4     4     4        4     4   \n",
       "7                 6     6          6      6     6     6        6     6   \n",
       "8                 2     2          2      2     2     2        2     2   \n",
       "...             ...   ...        ...    ...   ...   ...      ...   ...   \n",
       "435               1     1          1      1     1     1        1     1   \n",
       "439               1     1          1      1     1     1        1     1   \n",
       "450               1     1          1      1     1     1        1     1   \n",
       "491               1     1          1      1     1     1        1     1   \n",
       "495               1     1          1      1     1     1        1     1   \n",
       "\n",
       "        useful  funny  maxl  \n",
       "length                       \n",
       "2            5      5     5  \n",
       "3            4      4     4  \n",
       "6            4      4     4  \n",
       "7            6      6     6  \n",
       "8            2      2     2  \n",
       "...        ...    ...   ...  \n",
       "435          1      1     1  \n",
       "439          1      1     1  \n",
       "450          1      1     1  \n",
       "491          1      1     1  \n",
       "495          1      1     1  \n",
       "\n",
       "[278 rows x 11 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47f3c47",
   "metadata": {},
   "source": [
    "not much use batching sentences of same length :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "58158e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31    Disgusting!  Had a Groupon so my daughter and ...\n",
      "61    I have always been a fan of Burlington's deals...\n",
      "64    Another night meeting friends here.  I have to...\n",
      "65    Not busy at all but took nearly 45 min to get ...\n",
      "71    Yikes, reading other reviews I realize my bad ...\n",
      "Name: text, dtype: object\n",
      "31    1\n",
      "61    1\n",
      "64    1\n",
      "65    1\n",
      "71    1\n",
      "Name: stars, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Seperate the dataset into X and Y for prediction\n",
    "x = data_train['text']\n",
    "y = data_train['stars']\n",
    "print(x.head())\n",
    "print(y.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e04383a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_short = x.reset_index(drop=True)\n",
    "y_short = y.reset_index(drop=True)\n",
    "i=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ce5ae928",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def bert_encode(sentences):\n",
    "    toks = tokenizer.encode(sentences,return_tensors=\"pt\")\n",
    "    return model(toks[:,0:min(512,toks.shape[1])]).last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "69c59432",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_encode(x):\n",
    "    global i\n",
    "    i+=1\n",
    "    if i%50==0:\n",
    "        print(i)\n",
    "    return list(map(bert_encode, list( map( lambda x: x+\".\",x.split(\".\") ) ) )  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "7dd27e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_encode(x):\n",
    "    global i\n",
    "    i+=1\n",
    "    if i%50==0:\n",
    "        print(i)\n",
    "    return bert_encode(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "3f46815f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[126], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m embeddings\u001b[38;5;241m=\u001b[39m \u001b[43mx_short\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreg_encode\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m#embed entire review - so each row is a tensor\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\series.py:4631\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4521\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4522\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4523\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4526\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4527\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4528\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4529\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4530\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4629\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4630\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\apply.py:1025\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\apply.py:1076\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1074\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1075\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m-> 1076\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1077\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1078\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1079\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1080\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1082\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1083\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2834\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[125], line 6\u001b[0m, in \u001b[0;36mreg_encode\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m50\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbert_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[123], line 4\u001b[0m, in \u001b[0;36mbert_encode\u001b[1;34m(sentences)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad()\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbert_encode\u001b[39m(sentences):\n\u001b[0;32m      3\u001b[0m     toks \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(sentences,return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoks\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mtoks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlast_hidden_state\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1020\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1011\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m   1013\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m   1014\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1015\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1018\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[0;32m   1019\u001b[0m )\n\u001b[1;32m-> 1020\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1028\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1029\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1030\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1031\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1032\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1033\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:610\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    601\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[0;32m    602\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    603\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    607\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    608\u001b[0m     )\n\u001b[0;32m    609\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 610\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    615\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    618\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    620\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:495\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    483\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    484\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    485\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    492\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    494\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 495\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    502\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    504\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:425\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    416\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    417\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    423\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    424\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m--> 425\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    434\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[0;32m    435\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:353\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    350\u001b[0m     attention_scores \u001b[38;5;241m=\u001b[39m attention_scores \u001b[38;5;241m+\u001b[39m attention_mask\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# Normalize the attention scores to probabilities.\u001b[39;00m\n\u001b[1;32m--> 353\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;66;03m# This is actually dropping out entire tokens to attend to, which might\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;66;03m# seem a bit unusual, but is taken from the original Transformer paper.\u001b[39;00m\n\u001b[0;32m    357\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_probs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:1843\u001b[0m, in \u001b[0;36msoftmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1841\u001b[0m     dim \u001b[38;5;241m=\u001b[39m _get_softmax_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim(), _stacklevel)\n\u001b[0;32m   1842\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1843\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1844\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1845\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "embeddings= x_short.apply(reg_encode)#embed entire review - so each row is a tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db93782d",
   "metadata": {},
   "source": [
    "maxx = 0\n",
    "for j in range(999):\n",
    "    for i in embeddings[j]:\n",
    "        maxx = max(maxx, i.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05cf77a",
   "metadata": {},
   "source": [
    "print(maxx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7390c6a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 82, 768])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb64ce85",
   "metadata": {},
   "source": [
    "a failed attempt to do pos encoding before handing to the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e51d82",
   "metadata": {},
   "source": [
    "def pos_encode_list(lst, method):\n",
    "    \"\"\"returns Tensors in non-downsample case, list of tensors otherwise.\n",
    "    Had to rewrite so that classification token will get correct pos\"\"\"\n",
    "    list_len = len(lst)\n",
    "    dim = lst[0].shape[2]\n",
    "    global j\n",
    "    j+=1\n",
    "    if j%50==0:\n",
    "        print(j)\n",
    "    if method == \"1D\":\n",
    "        data = torch.cat(lst,1)\n",
    "        pos = torch.unsqueeze(get_positional_embeddings(data.shape[1]+1,data.shape[2])[1:],0)\n",
    "        return data + pos\n",
    "\n",
    "    elif method == \"2D\":\n",
    "        newlst=[]\n",
    "        assert dim%2 ==0\n",
    "        dim = dim//2\n",
    "        part_1 = get_positional_embeddings(list_len+1,dim)[1:]\n",
    "        for i, elem in enumerate(lst):\n",
    "            sublist_len = elem.shape[1]\n",
    "            part_2 = get_positional_embeddings(sublist_len,dim)\n",
    "            pos = torch.cat((part_1[i].expand(sublist_len,-1),part_2),1)\n",
    "            newlst.append(elem+torch.unsqueeze(pos,0))\n",
    "        return torch.cat(newlst,1)\n",
    "    \n",
    "    elif method == \"downsampling\":\n",
    "        new_lst = []\n",
    "        for i, elem in enumerate(lst):\n",
    "            pos = torch.unsqueeze(get_positional_embeddings(elem.shape[1]+1,elem.shape[2])[1:],0)\n",
    "            new_lst.append(pos+elem)\n",
    "        return new_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39279421",
   "metadata": {},
   "source": [
    "j=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce564059",
   "metadata": {},
   "source": [
    "catted = embeddings.apply(lambda x:pos_encode_list(x,\"1D\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd16fddd",
   "metadata": {},
   "source": [
    "maxx = 0\n",
    "for j in catted:\n",
    "    maxx = max(maxx, j.shape[1])#should hopefully be 200 - nope im stupid. didn't put limit on total rev length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48db1a35",
   "metadata": {},
   "source": [
    "maxx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ffa5f4",
   "metadata": {},
   "source": [
    "catted.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df809152",
   "metadata": {},
   "source": [
    "for i in embeddings[0]:\n",
    "    print( i.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78eb445b",
   "metadata": {},
   "source": [
    "get_positional_embeddings(100,10)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f850b1e",
   "metadata": {},
   "source": [
    "get_positional_embeddings(100,20)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "fd464088",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_pad_tens(lst,maxl):\n",
    "    \"\"\"for use in downsampled case\"\"\"\n",
    "    #max(list(map(lambda x: x.shape[1], lst)))\n",
    "    newl = []\n",
    "    for tens in lst:\n",
    "        pad = torch.zeros(tens.shape[0],maxl-tens.shape[1],tens.shape[2])\n",
    "        newt = torch.cat((tens, pad),1)\n",
    "        newl.append(newt)\n",
    "    return torch.cat(newl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92042663",
   "metadata": {},
   "source": [
    "list_to_pad_tens(embeddings[0]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe8c1e2",
   "metadata": {},
   "source": [
    "padded_cat = list_to_pad_tens(catted,900)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f41a70",
   "metadata": {},
   "source": [
    "padded_cat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824d44d4",
   "metadata": {},
   "source": [
    "embeddings_padded = embeddings.apply(lambda x: list_to_pad_tens(x, 200))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0337e5af",
   "metadata": {},
   "source": [
    "embeddings_padded[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b36e442a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(elem):\n",
    "    lst = [elem==i for i in [1,3,5]]\n",
    "    return torch.Tensor(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2534c2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proper = y_short.apply(to_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b065bd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_proper = y_short.apply(lambda x: x//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "74b8ab2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([1., 0., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 1., 0.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.]),\n",
       " tensor([0., 0., 1.])]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_proper.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "id": "ee2ad732",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# y_proper\n",
    "#embeddings_padded[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "259998de",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.concat((embeddings[0:300],embeddings[333:633],embeddings[666:966]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ac3c88dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.concat((y_proper[0:300],y_proper[333:633] ,y_proper[666:966]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ef94782e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = pd.concat((embeddings[300:333],embeddings[633:666],embeddings[966:999]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "30fa37c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = pd.concat((y_proper[300:333],y_proper[633:666] ,y_proper[966:999]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20424e87",
   "metadata": {},
   "source": [
    "#x_train.index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55568f99",
   "metadata": {},
   "source": [
    "Tensor(y_proper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4548b815",
   "metadata": {},
   "source": [
    "#x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f277dd",
   "metadata": {},
   "source": [
    "def train_split(X,Y):\n",
    "    y = torch.cat(list(map(lambda x: torch.unsqueeze(x,0),Y.tolist())))\n",
    "    x_train = torch.cat((X[0:300],X[333:633],X[666:966]))\n",
    "    y_train = torch.cat((y[0:300],y[333:633] ,y[666:966]))\n",
    "    x_test = torch.cat((X[300:333],X[633:666],X[966:999]))\n",
    "    y_test = torch.cat((y[300:333],y[633:666] ,y[966:999]))\n",
    "    return (x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d919e1",
   "metadata": {},
   "source": [
    "x_train, y_train, x_test, y_test = train_split(padded_cat,y_proper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b4174a",
   "metadata": {},
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f68f8a",
   "metadata": {},
   "source": [
    "y_train.sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "58d3390c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a4112f",
   "metadata": {},
   "source": [
    "class tensor_dataset(Dataset):\n",
    "    def __init__(self, s_x, s_y):\n",
    "        self.data = s_x\n",
    "        self.target = s_y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return torch.unsqueeze(self.data[idx],0), torch.unsqueeze(self.target[idx],0).long()\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ffc04dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class series_dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, s_x, s_y):\n",
    "        self.data = s_x\n",
    "        self.target = s_y\n",
    "        self.index = s_x.index\n",
    "        assert( (s_x.index==s_y.index).all() )\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.index)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        idd = self.index[idx]\n",
    "        return self.data[idd], torch.Tensor(self.target[idd]).long()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2f7ad771",
   "metadata": {},
   "outputs": [],
   "source": [
    "series_data = series_dataset(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5052d412",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = series_dataset(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f98e2a5",
   "metadata": {},
   "source": [
    "series_data[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73db49ac",
   "metadata": {},
   "source": [
    "series_data[0][1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb80fb7d",
   "metadata": {},
   "source": [
    "((series_data[0][0]!=0)*series_data[0][0]==series_data[0][0]).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348bb6a0",
   "metadata": {},
   "source": [
    "torch.nonzero(series_data[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b899ab7",
   "metadata": {},
   "source": [
    "train_ds = tensor_dataset(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b0eb4e",
   "metadata": {},
   "source": [
    "test_ds = tensor_dataset(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb935896",
   "metadata": {},
   "source": [
    "past here needs a good look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "67d2006c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0+cu118\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math,copy,re\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.simplefilter(\"ignore\")\n",
    "print(torch.__version__)\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af73831b",
   "metadata": {},
   "source": [
    "((series_data[0][0]!=0)*series_data[0][0] # main masking idea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126b26a3",
   "metadata": {},
   "source": [
    "train_ds[0:2][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d09f77b",
   "metadata": {},
   "source": [
    "torch.sum((train_ds[0:2][0]!=0),dim=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424445d4",
   "metadata": {},
   "source": [
    "train_ds[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cb06b316",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positional_embeddings(sequence_length, d): #\"i\" in paper corresponds to j - i.e. along dimension of size d\n",
    "    result = torch.ones(sequence_length, d) #pos in paper refers to which token - i.e. varying from 1 to 50\n",
    "    for i in range(sequence_length):\n",
    "      for j in range(d):\n",
    "        if j%2==0:\n",
    "          result[i,j] = math.sin(i/10000**(j/d))\n",
    "        else:\n",
    "          result[i,j] = math.cos(i/10000**((j-1)/d))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8d08fdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, pos_method, input_dim, hidden_d, out_d, n_heads, n_blocks):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.pos_method = pos_method\n",
    "        \n",
    "        #self.class_token = nn.Parameter(torch.rand((input_dim)))\n",
    "        \n",
    "        self.linear = nn.Linear(input_dim, hidden_d)\n",
    "        \n",
    "        self.blocks = nn.ModuleList([MyViTBlock(hidden_d, n_heads) for _ in range(n_blocks)])\n",
    "        \n",
    "        self.mlp = nn.Linear(hidden_d,out_d)#for CELoss\n",
    "        #nn.Sequential(nn.Linear(hidden_d,out_d), nn.Softmax(dim=-1))\n",
    "        \n",
    "    def forward(self, sentence):\n",
    "        #print(sentence.shape)\n",
    "        pos = torch.unsqueeze(get_positional_embeddings(sentence.shape[-2], sentence.shape[-1]).to(device),0)\n",
    "        #out = torch.cat((torch.unsqueeze(self.class_token+token_pos,0),sentence[0]),1)\n",
    "        #print(sentence.shape)\n",
    "        out = sentence[0]\n",
    "        out = out+pos\n",
    "        out = self.linear(out) #so input is now hidden dim shape, i.e.1,s+1, h_d\n",
    "        for block in self.blocks:\n",
    "            out = block(out)    \n",
    "        out = torch.mean(out,dim=1)\n",
    "        \n",
    "        return torch.unsqueeze(self.mlp(out),0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e921c264",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMSA(nn.Module):\n",
    "    \"\"\"MSA block\"\"\"\n",
    "    def __init__(self, d, n_heads=2):#d is hidden dim\n",
    "        super(MyMSA, self).__init__()\n",
    "        self.d = d\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        assert d % n_heads == 0, f\"Can't divide dimension {d} into {n_heads} heads\"\n",
    "\n",
    "        d_head = int(d / n_heads) #dim of each head\n",
    "        self.q_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n",
    "        self.k_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n",
    "        self.v_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n",
    "        self.d_head = d_head\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, sequences):\n",
    "        # Sequences has shape (N, seq_length, token_dim)\n",
    "        # We go into shape    (N, seq_length, n_heads, token_dim / n_heads)\n",
    "        # And come back to    (N, seq_length, item_dim)  (through concatenation)\n",
    "        result = []\n",
    "        for sequence in sequences:\n",
    "            seq_result = []\n",
    "            for head in range(self.n_heads):\n",
    "                q_mapping = self.q_mappings[head]\n",
    "                k_mapping = self.k_mappings[head]\n",
    "                v_mapping = self.v_mappings[head]\n",
    "                \n",
    "                #print(f\"{head=}\")\n",
    "\n",
    "                seq = sequence[:, head * self.d_head: (head + 1) * self.d_head] #interesting? so each attention head only looks at a subset of features\n",
    "                #print(seq.shape)\n",
    "                q, k, v = q_mapping(seq), k_mapping(seq), v_mapping(seq)\n",
    "\n",
    "                attention = self.softmax(q @ k.T / math.sqrt(self.d)) # here we take dot product between q and k vectors\n",
    "                seq_result.append(attention @ v) #and here we do a weighted sum over v vectors based on attentions\n",
    "            result.append(torch.hstack(seq_result))\n",
    "        return torch.cat([torch.unsqueeze(r, dim=0) for r in result])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b3a35323",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyViTBlock(nn.Module):\n",
    "    def __init__(self, hidden_d, n_heads, mlp_ratio=4):\n",
    "        super(MyViTBlock, self).__init__()\n",
    "        self.hidden_d = hidden_d\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(hidden_d)\n",
    "        self.mhsa = MyMSA(hidden_d, n_heads)\n",
    "        self.norm2 = nn.LayerNorm(hidden_d)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_d, mlp_ratio * hidden_d),#i.e. mlp ratio tells us how much bigger mlp hidden is than previous hidden\n",
    "            nn.GELU(),\n",
    "            nn.Linear(mlp_ratio * hidden_d, hidden_d)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x + self.mhsa(self.norm1(x)) #so we do residual on multi self attention\n",
    "        out = out + self.mlp(self.norm2(out)) #then residual on mlp\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3632c2c",
   "metadata": {},
   "source": [
    "t = Transformer(pos_method = \"1D\", input_dim=768, hidden_d=512, out_d=3, n_heads = 4,n_blocks = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cb80b4",
   "metadata": {},
   "source": [
    "t(series_data[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26566db5",
   "metadata": {},
   "source": [
    "tt = tiered_transformer().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ceda6b",
   "metadata": {},
   "source": [
    "tt(torch.unsqueeze(series_data[5][0],0).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1a485b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#t(series_data[0][0]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303dccb5",
   "metadata": {},
   "source": [
    "training now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3d030726",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def train(net, dataloader, optim, loss_func, epoch):\n",
    "    net.train()  #Put the network in train mode\n",
    "    total_loss = 0\n",
    "    batches = 0\n",
    "    start = time.time()\n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        batches += 1\n",
    "\n",
    "        # Training loop\n",
    "        optim.zero_grad() # to clear gradient in optimizer - this is equivalent to net.zero_grad().\n",
    "\n",
    "        pred = net(data.to(device))[0]\n",
    "#         print(f\"{pred.shape=}\")        \n",
    "#         print(f\"{target.shape=}\")\n",
    "        loss = loss_func(pred,target.to(device).float())\n",
    "\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        total_loss += loss\n",
    "        if batch_idx % 10 == 0: #Report stats every x batches\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, (batch_idx+1) * len(data), len(dataloader.dataset),\n",
    "                           100. * (batch_idx+1) / len(dataloader), loss.item()), flush=True)\n",
    "    av_loss = total_loss / batches\n",
    "    av_loss = av_loss.detach().cpu().numpy()\n",
    "    print('\\nTraining set: Average loss: {:.4f}'.format(av_loss,  flush=True))\n",
    "    total_time = time.time() - start\n",
    "    print('Time taken for epoch = ', total_time)\n",
    "    return av_loss\n",
    "\n",
    "def val(net, val_dataloader, optim, loss_func, epoch):\n",
    "    net.eval()  #Put the model in eval mode\n",
    "    total_loss = 0    \n",
    "    batches = 0\n",
    "    with torch.no_grad():  # So no gradients accumulate\n",
    "        for batch_idx, (data, target) in enumerate(val_dataloader):\n",
    "            batches += 1\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            #Eval steps\n",
    "            optim.zero_grad() # to clear gradient in optimizer - this is equivalent to net.zero_grad().\n",
    "\n",
    "            pred = net(data.to(device))\n",
    "            loss = loss_func(pred,target.to(device).float())\n",
    "\n",
    "            total_loss += loss\n",
    "        av_loss = total_loss / batches\n",
    "        \n",
    "    av_loss = av_loss.detach().cpu().numpy()\n",
    "    print('Validation set: Average loss: {:.4f}'.format(av_loss,  flush=True))\n",
    "    print('\\n')\n",
    "    return av_loss\n",
    "\n",
    "def predict(net, test_dataloader):\n",
    "    pred_store = []\n",
    "    true_store = []\n",
    "    batches = 0\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():  # So no gradients accumulate\n",
    "        for batch_idx, (data, target) in enumerate(val_dataloader):\n",
    "            batches += 1\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            #Eval steps\n",
    "            optim.zero_grad() # to clear gradient in optimizer - this is equivalent to net.zero_grad().\n",
    "\n",
    "            pred = net(data.to(device))\n",
    "            loss = class_loss(pred,target.to(device).float())\n",
    "\n",
    "            total_loss += loss\n",
    "\n",
    "            pred_store.append(np.argmax(pred.detach().cpu().numpy(), axis=1)) #along each pixel, take argmax - i.e. whether to put 0/1\n",
    "            true_store.append(np.argmax(target.detach().cpu().numpy(), axis=1))\n",
    "\n",
    "        av_loss = total_loss / batches\n",
    "        \n",
    "    av_loss = av_loss.detach().cpu().numpy()\n",
    "    pred_store = np.array(pred_store).reshape(-1)\n",
    "    true_store = np.array(true_store).reshape(-1)\n",
    "    acc = accuracy_score(pred_store, true_store)\n",
    "\n",
    "\n",
    "    print('Test set: Average loss: {:.4f}'.format(av_loss,  flush=True))\n",
    "    print('Test set: Average Acc: {:.4f}'.format(acc,  flush=True))\n",
    "    print(\"\\n\")\n",
    "    return pred_store, true_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "028e71e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(series_data, batch_size=1, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c7ea7e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net(torch.unsqueeze(series_data[0][0],0)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "16cd9df5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series_data[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "674bf368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "018ece4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7e4ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Transformer(pos_method = \"1D\", input_dim=768, hidden_d=512, out_d=3, n_heads = 4,n_blocks = 4)#\n",
    "net = net.to(device)\n",
    "#Calculate the number of traininable params\n",
    "print('Trainable params: ', net.parameters())\n",
    "\n",
    "class_loss = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.Adam(net.parameters())#Adam makes diff?\n",
    "\n",
    "losses = []\n",
    "max_epochs = 100\n",
    "for epoch in range(1, max_epochs+1):\n",
    "    train_loss = train(net, train_dataloader, optim, class_loss, epoch)\n",
    "    losses.append([train_loss])\n",
    "\n",
    "losses = np.array(losses).T\n",
    "print(losses.shape)\n",
    "its = np.linspace(1, max_epochs, max_epochs)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(its, losses[0,:])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "31ed276e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "sentences = list_encode(x_short[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5203c64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list_to_pad_tens(sentences,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "683810bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 5, 50, 768])\n"
     ]
    }
   ],
   "source": [
    "sentences = torch.unsqueeze(sentences,0)\n",
    "print(sentences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "10b194cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 50, 768])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5c3bb941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([17, 25,  9, 37,  3])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum((sentences[0]!=0),dim=1)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "2e5afdd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([17, 25,  9, 37,  3])\n",
      "torch.Size([17, 768])\n",
      "torch.Size([25, 768])\n",
      "torch.Size([9, 768])\n",
      "torch.Size([37, 768])\n",
      "torch.Size([3, 768])\n"
     ]
    }
   ],
   "source": [
    "list_len = sentences.shape[1]\n",
    "dim = sentences.shape[-1]\n",
    "lengths = torch.sum((sentences[0]!=0),dim=1)[:,0]#get lengths of each sentence\n",
    "print(lengths)\n",
    "lst = []\n",
    "for i,elem in enumerate(sentences[0]):\n",
    "    elem = elem[0:lengths[i]]\n",
    "    print(elem.shape)\n",
    "    fw = elem[0]\n",
    "    lst.append(fw)    \n",
    "\n",
    "fws = torch.cat(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "06dd65ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3840])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fws.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "58c1a14d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_len = sentences.shape[1]\n",
    "dim = sentences.shape[-1]\n",
    "lengths = torch.sum((sentences[0]!=0),dim=1)[:,0]#get lengths of each sentence\n",
    "\n",
    "newlst=[]\n",
    "assert dim%2 ==0\n",
    "dim = dim//2\n",
    "part_1 = get_positional_embeddings(list_len,dim)\n",
    "\n",
    "for i, elem in enumerate(sentences[0]):\n",
    "    sublist_len = lengths[i]\n",
    "    part_2 = get_positional_embeddings(sublist_len,dim)\n",
    "    pos = torch.cat((part_1[i].expand(sublist_len,-1),part_2),1)\n",
    "    newlst.append(elem[0:sublist_len]+torch.unsqueeze(pos,0))\n",
    "x=torch.cat(newlst,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "22b3520d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 91, 768])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "id": "2fd77f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#net(torch.unsqueeze(x_test[0],0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "id": "b5497568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 1.])"
      ]
     },
     "execution_count": 662,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "id": "06bab5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[6.6923e-04, 3.5589e-03, 9.9577e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[0.0011, 0.0092, 0.9898]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[5.8590e-04, 3.6044e-03, 9.9581e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[6.6804e-04, 4.8065e-03, 9.9453e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0.])\n",
      "tensor([[[0.0014, 0.0142, 0.9844]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[0.0061, 0.0589, 0.9351]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0.])\n",
      "tensor([[[9.8277e-04, 4.3733e-03, 9.9464e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[0.0028, 0.0285, 0.9687]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[9.2137e-04, 7.3226e-03, 9.9176e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0.])\n",
      "tensor([[[7.7201e-04, 5.7869e-03, 9.9344e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1., 0., 0.])\n",
      "tensor([[[0.0010, 0.0091, 0.9899]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[6.0114e-04, 4.1037e-03, 9.9530e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0.])\n",
      "tensor([[[0.0012, 0.0111, 0.9877]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[9.1899e-04, 7.3283e-03, 9.9175e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1., 0., 0.])\n",
      "tensor([[[7.7127e-04, 5.7891e-03, 9.9344e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[6.6689e-04, 4.8048e-03, 9.9453e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[0.0055, 0.0542, 0.9402]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[6.0388e-04, 4.1173e-03, 9.9528e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0.])\n",
      "tensor([[[6.6545e-04, 4.8007e-03, 9.9453e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1., 0., 0.])\n",
      "tensor([[[6.0475e-04, 4.1101e-03, 9.9529e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1., 0., 0.])\n",
      "tensor([[[0.0514, 0.4316, 0.5170]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0.])\n",
      "tensor([[[0.0522, 0.4385, 0.5093]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0.])\n",
      "tensor([[[0.0101, 0.0792, 0.9107]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0.])\n",
      "tensor([[[0.0010, 0.0091, 0.9899]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[5.9833e-04, 4.0922e-03, 9.9531e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[6.0038e-04, 4.0861e-03, 9.9531e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[0.0014, 0.0142, 0.9844]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[7.7082e-04, 5.7705e-03, 9.9346e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[7.6786e-04, 5.7846e-03, 9.9345e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[9.9267e-04, 4.4044e-03, 9.9460e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[0.0014, 0.0142, 0.9844]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1., 0., 0.])\n",
      "tensor([[[9.8446e-04, 4.3718e-03, 9.9464e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[0.0069, 0.0647, 0.9284]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[7.6816e-04, 5.7948e-03, 9.9344e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[0.0364, 0.3099, 0.6537]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0.])\n",
      "tensor([[[0.0014, 0.0143, 0.9843]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[5.9956e-04, 4.0942e-03, 9.9531e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0.])\n",
      "tensor([[[7.6699e-04, 5.7712e-03, 9.9346e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1., 0., 0.])\n",
      "tensor([[[0.0019, 0.0194, 0.9788]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0.])\n",
      "tensor([[[0.0126, 0.0961, 0.8913]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0.])\n",
      "tensor([[[0.0028, 0.0281, 0.9692]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[0.0101, 0.0790, 0.9109]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0.])\n",
      "tensor([[[0.0100, 0.0787, 0.9113]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0.])\n",
      "tensor([[[6.6258e-04, 4.7911e-03, 9.9455e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[5.8115e-04, 3.5902e-03, 9.9583e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[9.1454e-04, 7.3064e-03, 9.9178e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[0.0014, 0.0142, 0.9844]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0.])\n",
      "tensor([[[6.6610e-04, 4.8076e-03, 9.9453e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0.])\n",
      "tensor([[[0.0109, 0.0841, 0.9050]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1., 0., 0.])\n",
      "tensor([[[0.0068, 0.0641, 0.9291]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0.])\n",
      "tensor([[[5.9989e-04, 4.0903e-03, 9.9531e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[0.0094, 0.0783, 0.9122]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0.])\n",
      "tensor([[[0.0069, 0.0648, 0.9283]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0.])\n",
      "tensor([[[0.0050, 0.0488, 0.9463]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0.])\n",
      "tensor([[[6.6523e-04, 4.7946e-03, 9.9454e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[9.1515e-04, 7.3268e-03, 9.9176e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0.])\n",
      "tensor([[[5.8274e-04, 3.6043e-03, 9.9581e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[0.0055, 0.0541, 0.9404]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[0.0010, 0.0044, 0.9946]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[0.0081, 0.0719, 0.9201]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[0.0403, 0.3504, 0.6093]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[5.9479e-04, 4.0976e-03, 9.9531e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0.])\n",
      "tensor([[[7.7201e-04, 5.8050e-03, 9.9342e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[0.0112, 0.0860, 0.9028]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[6.0162e-04, 4.1031e-03, 9.9530e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[7.7038e-04, 5.8019e-03, 9.9343e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[0.0019, 0.0193, 0.9789]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[0.0027, 0.0280, 0.9693]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[7.6782e-04, 5.7637e-03, 9.9347e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[0.0012, 0.0112, 0.9877]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[0.0012, 0.0112, 0.9876]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[0.0566, 0.4979, 0.4455]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0.])\n",
      "tensor([[[9.8902e-04, 4.3936e-03, 9.9462e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[0.0061, 0.0592, 0.9347]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0.])\n",
      "tensor([[[0.0011, 0.0091, 0.9898]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0.])\n",
      "tensor([[[9.1778e-04, 7.3217e-03, 9.9176e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0.])\n",
      "tensor([[[0.0268, 0.2232, 0.7500]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0.])\n",
      "tensor([[[0.0011, 0.0091, 0.9898]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[0.0028, 0.0283, 0.9690]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[5.8642e-04, 3.6051e-03, 9.9581e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[0.0099, 0.0783, 0.9117]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1., 0., 0.])\n",
      "tensor([[[0.0011, 0.0091, 0.9898]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[6.0209e-04, 4.1016e-03, 9.9530e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[9.8687e-04, 4.3773e-03, 9.9464e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[0.0019, 0.0196, 0.9786]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0.])\n",
      "tensor([[[0.0080, 0.0711, 0.9210]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[6.0077e-04, 4.1073e-03, 9.9529e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[0.0068, 0.0640, 0.9292]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1., 0., 0.])\n",
      "tensor([[[0.0148, 0.1129, 0.8723]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1., 0., 0.])\n",
      "tensor([[[0.0012, 0.0112, 0.9876]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[0.0040, 0.0399, 0.9561]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0.])\n",
      "tensor([[[9.1812e-04, 7.3489e-03, 9.9173e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0.])\n",
      "tensor([[[0.0014, 0.0142, 0.9844]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[0.0012, 0.0113, 0.9875]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[6.6704e-04, 3.5353e-03, 9.9580e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0.])\n",
      "tensor([[[0.0525, 0.4342, 0.5132]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0.])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[5.8564e-04, 3.6074e-03, 9.9581e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[5.8166e-04, 3.5891e-03, 9.9583e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[0.0570, 0.4983, 0.4447]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[5.8351e-04, 3.5932e-03, 9.9582e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[9.8937e-04, 4.4028e-03, 9.9461e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[0.0055, 0.0541, 0.9404]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1., 0., 0.])\n",
      "tensor([[[0.0012, 0.0112, 0.9877]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[0.0069, 0.0647, 0.9284]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1., 0., 0.])\n",
      "tensor([[[0.0509, 0.4091, 0.5400]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[9.1766e-04, 7.3161e-03, 9.9177e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[0.0012, 0.0112, 0.9877]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[0.0109, 0.0843, 0.9047]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1., 0., 0.])\n",
      "tensor([[[0.0056, 0.0549, 0.9395]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[0.0012, 0.0112, 0.9876]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[0.0012, 0.0112, 0.9876]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0.])\n",
      "tensor([[[5.8421e-04, 3.5931e-03, 9.9582e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1., 0., 0.])\n",
      "tensor([[[0.0014, 0.0141, 0.9845]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[6.6349e-04, 4.7969e-03, 9.9454e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0.])\n",
      "tensor([[[0.0012, 0.0112, 0.9876]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[7.7319e-04, 5.8019e-03, 9.9342e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0.])\n",
      "tensor([[[6.6621e-04, 4.7979e-03, 9.9454e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1., 0., 0.])\n",
      "tensor([[[0.0069, 0.0649, 0.9283]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[0.0100, 0.0786, 0.9114]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[9.8616e-04, 4.3925e-03, 9.9462e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[0.0099, 0.0771, 0.9130]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[6.0141e-04, 4.1011e-03, 9.9530e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[0.0011, 0.0091, 0.9898]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[7.7108e-04, 5.7908e-03, 9.9344e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[0.0093, 0.0781, 0.9126]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[0.0028, 0.0282, 0.9691]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1., 0., 0.])\n",
      "tensor([[[7.7013e-04, 5.7935e-03, 9.9344e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[6.0102e-04, 4.0868e-03, 9.9531e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0.])\n",
      "tensor([[[6.6563e-04, 3.5280e-03, 9.9581e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0.])\n",
      "tensor([[[9.1577e-04, 7.3106e-03, 9.9177e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0.])\n",
      "tensor([[[0.0102, 0.0791, 0.9108]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[7.6972e-04, 5.7916e-03, 9.9344e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[0.0040, 0.0400, 0.9560]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0.])\n",
      "tensor([[[0.0010, 0.0091, 0.9898]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[0.0012, 0.0112, 0.9876]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[0.0019, 0.0195, 0.9786]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0.])\n",
      "tensor([[[0.0014, 0.0142, 0.9844]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0.])\n",
      "tensor([[[9.2222e-04, 7.3474e-03, 9.9173e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[6.6590e-04, 4.7869e-03, 9.9455e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[7.7003e-04, 5.7883e-03, 9.9344e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[7.7205e-04, 5.8134e-03, 9.9341e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1., 0., 0.])\n",
      "tensor([[[0.0069, 0.0648, 0.9283]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[0.0010, 0.0091, 0.9899]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[7.6766e-04, 5.7595e-03, 9.9347e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[0.0011, 0.0091, 0.9898]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0.])\n",
      "tensor([[[6.6087e-04, 3.5119e-03, 9.9583e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[0.0028, 0.0284, 0.9688]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[5.8568e-04, 3.6039e-03, 9.9581e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1., 0., 0.])\n",
      "tensor([[[0.0011, 0.0091, 0.9898]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0.])\n",
      "tensor([[[6.6365e-04, 4.7879e-03, 9.9455e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0.])\n",
      "tensor([[[6.0206e-04, 4.1044e-03, 9.9529e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[0.0027, 0.0280, 0.9692]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[5.8386e-04, 3.5964e-03, 9.9582e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[0.0126, 0.0956, 0.8918]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[0.0012, 0.0112, 0.9876]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[6.6758e-04, 4.8128e-03, 9.9452e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[6.6649e-04, 4.7940e-03, 9.9454e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0.])\n",
      "tensor([[[9.8619e-04, 4.4202e-03, 9.9459e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[0.0012, 0.0112, 0.9876]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[9.1659e-04, 7.3084e-03, 9.9177e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[0.0014, 0.0143, 0.9842]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0.])\n",
      "tensor([[[0.0061, 0.0588, 0.9351]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0.])\n",
      "tensor([[[0.0049, 0.0488, 0.9463]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0.])\n",
      "tensor([[[9.8416e-04, 4.3879e-03, 9.9463e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[6.0182e-04, 4.1071e-03, 9.9529e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0.])\n",
      "tensor([[[5.8401e-04, 3.6002e-03, 9.9582e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[6.6892e-04, 3.5505e-03, 9.9578e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[9.2319e-04, 7.3448e-03, 9.9173e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0.])\n",
      "tensor([[[0.0014, 0.0143, 0.9843]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0.])\n",
      "tensor([[[0.0028, 0.0281, 0.9691]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1., 0., 0.])\n",
      "tensor([[[0.0050, 0.0491, 0.9459]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[0.0101, 0.0787, 0.9113]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1., 0., 0.])\n",
      "tensor([[[0.0056, 0.0548, 0.9396]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[0.0102, 0.0793, 0.9105]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[0.0012, 0.0111, 0.9877]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1., 0., 0.])\n",
      "tensor([[[0.0010, 0.0091, 0.9898]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[6.6669e-04, 3.5202e-03, 9.9581e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0.])\n",
      "tensor([[[0.0040, 0.0404, 0.9556]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[0.0010, 0.0090, 0.9899]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[0.0061, 0.0595, 0.9344]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[0.0050, 0.0491, 0.9459]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0.])\n",
      "tensor([[[5.8757e-04, 3.6041e-03, 9.9581e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1., 0., 0.])\n",
      "tensor([[[6.6439e-04, 4.7891e-03, 9.9455e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[0.0567, 0.4960, 0.4473]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[6.6646e-04, 4.8049e-03, 9.9453e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0.])\n",
      "tensor([[[9.1107e-04, 7.3075e-03, 9.9178e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[0.0218, 0.1809, 0.7973]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1., 0., 0.])\n",
      "tensor([[[0.0014, 0.0142, 0.9844]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1., 0., 0.])\n",
      "tensor([[[0.0110, 0.0848, 0.9042]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1., 0., 0.])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[7.6808e-04, 5.8018e-03, 9.9343e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[0.0530, 0.4401, 0.5069]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[0.0514, 0.4305, 0.5181]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0.])\n",
      "tensor([[[5.8684e-04, 3.6000e-03, 9.9581e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[6.6812e-04, 3.5454e-03, 9.9579e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[0.0050, 0.0494, 0.9455]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0.])\n",
      "tensor([[[0.0100, 0.0802, 0.9098]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1., 0., 0.])\n",
      "tensor([[[0.0069, 0.0647, 0.9284]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[0.0056, 0.0549, 0.9394]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[[0.0056, 0.0551, 0.9393]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0.])\n",
      "tensor([[[9.8696e-04, 4.3726e-03, 9.9464e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(y_test)):\n",
    "    print(net(torch.unsqueeze(x_test[y_test.index[i]],0)))\n",
    "    print(y_test[y_test.index[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "id": "f59ca402",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_dl, batch_size=1, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "id": "bf005262",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Long but found Float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[667], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[666], line 70\u001b[0m, in \u001b[0;36mpredict\u001b[1;34m(net, test_dataloader)\u001b[0m\n\u001b[0;32m     67\u001b[0m optim\u001b[38;5;241m.\u001b[39mzero_grad() \u001b[38;5;66;03m# to clear gradient in optimizer - this is equivalent to net.zero_grad().\u001b[39;00m\n\u001b[0;32m     69\u001b[0m pred \u001b[38;5;241m=\u001b[39m net(data\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m---> 70\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mclass_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[0;32m     74\u001b[0m pred_store\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39margmax(pred\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;66;03m#along each pixel, take argmax - i.e. whether to put 0/1\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1175\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1176\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:3029\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3027\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3028\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3029\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: expected scalar type Long but found Float"
     ]
    }
   ],
   "source": [
    "predict(net,test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4789bc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43b1772",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cbec442b",
   "metadata": {},
   "source": [
    "class tiered_transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_c = 768, out_c = 16, n_blocks=2, n_heads=2, hid_c=64, pos_d=64,pos_m = \"add\",bet = None):\n",
    "        super(tiered_transformer, self).__init__()\n",
    "        self.layer1 = transformer(in_c, out_c, n_blocks, n_heads, hid_c, pos_d,pos_m = \"add\",bet = None)\n",
    "        self.layer2 = transformer(out_c, 3 , 2, 2, 16, 16, pos_m = \"add\",bet = None)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print(f\"{x.shape=}\")\n",
    "        x = torch.unsqueeze(self.layer1(x),0)\n",
    "        #print(f\"{x.shape=}\")\n",
    "        x = self.layer2(x)\n",
    "        #print(f\"{x.shape=}\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014428dc",
   "metadata": {},
   "source": [
    "t = transformer(pos_m=\"cat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92eac76c",
   "metadata": {},
   "source": [
    "m = MHSABlock(128,2,4,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505830dc",
   "metadata": {},
   "source": [
    "#t(torch.unsqueeze(series_data[5][0],0)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc9aee5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tokenizers import Tokenizer\n",
    "# from tokenizers.models import WordPiece\n",
    "# bert_tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "\n",
    "#data[\"text\"].iloc[6]\n",
    "#yelp[\"text\"] = yelp[\"text\"].str.replace(\"\\n\",' ')\n",
    "#strs= yelp[\"text\"].iloc[6].replace(\"!\",\".\").split(\".\")\n",
    "#[x for x in strs if x]\n",
    "#pre_tokenizer.pre_tokenize_str(yelp[\"text\"].iloc[6])\n",
    "\n",
    "# # CLEANING THE REVIEWS - REMOVAL OF STOPWORDS AND PUNCTUATION\n",
    "# def text_process(text):\n",
    "#     nopunc = [char for char in text if char not in string.punctuation]\n",
    "#     nopunc = ''.join(nopunc)\n",
    "#     return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]\n",
    "\n",
    "# # CONVERTING THE WORDS INTO A VECTOR\n",
    "# vocab = CountVectorizer(analyzer=text_process).fit(x)\n",
    "# print(len(vocab.vocabulary_))\n",
    "# r0 = x[0]\n",
    "# print(r0)\n",
    "# vocab0 = vocab.transform([r0])\n",
    "# print(vocab0)\n",
    "# \"\"\"\n",
    "#     Now the words in the review number 78 have been converted into a vector.\n",
    "#     The data that we can see is the transformed words.\n",
    "#     If we now get the feature's name - we can get the word back!\n",
    "# \"\"\"\n",
    "# print(\"Getting the words back:\")\n",
    "# print(vocab.get_feature_names()[19648])\n",
    "# print(vocab.get_feature_names()[10643])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b84dcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tokenizer.encode(\"\\nbanana\",return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86049411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101, 15212]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[:,0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8460fc31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 768])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(tokenizer.encode(\"banana\",return_tensors=\"pt\")).last_hidden_state.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

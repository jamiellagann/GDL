{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e6f8952",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import numpy as np\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import math\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn # Contains all the functions we need to to train our network\n",
    "import torch.nn.functional as F # Contains some additional functions such as activations\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c4b199c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positional_embeddings(sequence_length, d): #\"i\" in paper corresponds to j - i.e. along dimension of size d\n",
    "    result = torch.ones(sequence_length, d) #pos in paper refers to which token - i.e. varying from 1 to 50\n",
    "    for i in range(sequence_length):\n",
    "      for j in range(d):\n",
    "        if j%2==0:\n",
    "          result[i,j] = math.sin(i/10000**(j/d))\n",
    "        else:\n",
    "          result[i,j] = math.cos(i/10000**((j-1)/d))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45285dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMSA(nn.Module):\n",
    "    \"\"\"MSA block\"\"\"\n",
    "    def __init__(self, d, n_heads=2):#d is hidden dim\n",
    "        super(MyMSA, self).__init__()\n",
    "        self.d = d\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        assert d % n_heads == 0, f\"Can't divide dimension {d} into {n_heads} heads\"\n",
    "\n",
    "        d_head = int(d / n_heads) #dim of each head\n",
    "        self.q_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n",
    "        self.k_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n",
    "        self.v_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n",
    "        self.d_head = d_head\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, sequences):\n",
    "        # Sequences has shape (N, seq_length, token_dim)\n",
    "        # We go into shape    (N, seq_length, n_heads, token_dim / n_heads)\n",
    "        # And come back to    (N, seq_length, item_dim)  (through concatenation)\n",
    "        result = []\n",
    "        for sequence in sequences:\n",
    "            seq_result = []\n",
    "            for head in range(self.n_heads):\n",
    "                q_mapping = self.q_mappings[head]\n",
    "                k_mapping = self.k_mappings[head]\n",
    "                v_mapping = self.v_mappings[head]\n",
    "                \n",
    "                #print(f\"{head=}\")\n",
    "\n",
    "                seq = sequence[:, head * self.d_head: (head + 1) * self.d_head] #interesting? so each attention head only looks at a subset of features\n",
    "                #print(seq.shape)\n",
    "                q, k, v = q_mapping(seq), k_mapping(seq), v_mapping(seq)\n",
    "\n",
    "                attention = self.softmax(q @ k.T / math.sqrt(self.d)) # here we take dot product between q and k vectors\n",
    "                seq_result.append(attention @ v) #and here we do a weighted sum over v vectors based on attentions\n",
    "            result.append(torch.hstack(seq_result))\n",
    "        return torch.cat([torch.unsqueeze(r, dim=0) for r in result])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "110667f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyViTBlock(nn.Module):\n",
    "    def __init__(self, hidden_d, n_heads, mlp_ratio=4):\n",
    "        super(MyViTBlock, self).__init__()\n",
    "        self.hidden_d = hidden_d\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(hidden_d)\n",
    "        self.mhsa = MyMSA(hidden_d, n_heads)\n",
    "        self.norm2 = nn.LayerNorm(hidden_d)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_d, mlp_ratio * hidden_d),#i.e. mlp ratio tells us how much bigger mlp hidden is than previous hidden\n",
    "            nn.GELU(),\n",
    "            nn.Linear(mlp_ratio * hidden_d, hidden_d)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x + self.mhsa(self.norm1(x)) #so we do residual on multi self attention\n",
    "        out = out + self.mlp(self.norm2(out)) #then residual on mlp\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d619ba1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer_2D_pos(nn.Module):\n",
    "    def __init__(self, pos_method, input_dim, hidden_d, out_d, n_heads, n_blocks):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.pos_method = pos_method\n",
    "        \n",
    "        self.class_token = nn.Parameter(torch.rand((input_dim)))\n",
    "        \n",
    "        self.linear = nn.Linear(input_dim, hidden_d)\n",
    "        \n",
    "        self.blocks = nn.ModuleList([MyViTBlock(hidden_d, n_heads) for _ in range(n_blocks)])\n",
    "        \n",
    "        self.mlp = nn.Linear(hidden_d,out_d)#for CELoss\n",
    "        #nn.Sequential(nn.Linear(hidden_d,out_d), nn.Softmax(dim=-1))\n",
    "        \n",
    "    def forward(self, sentences):\n",
    "        #input 1 x N x padded x 768\n",
    "        list_len = sentences.shape[1]\n",
    "        dim = sentences.shape[-1]\n",
    "        lengths = torch.sum((sentences[0]!=0),dim=1)[:,0]#get lengths of each sentence\n",
    "\n",
    "        newlst=[]\n",
    "        assert dim%2 ==0\n",
    "        dim = dim//2\n",
    "        part_1 = get_positional_embeddings(list_len,dim)\n",
    "\n",
    "        for i, elem in enumerate(sentences[0]):#doing dual encoding\n",
    "            sublist_len = lengths[i]\n",
    "            part_2 = get_positional_embeddings(sublist_len,dim)\n",
    "            pos = torch.cat((part_1[i].expand(sublist_len,-1),part_2),1)\n",
    "            newlst.append(elem[0:sublist_len]+torch.unsqueeze(pos,0))\n",
    "        out = torch.cat(newlst,1)\n",
    "        \n",
    "        out = self.linear(out) #so input is now hidden dim shape, i.e.1,s, h_d\n",
    "        for block in self.blocks:\n",
    "            out = block(out)    \n",
    "        out = out[:,0]\n",
    "        \n",
    "        return torch.unsqueeze(self.mlp(out),0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

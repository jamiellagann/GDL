{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f126b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import math\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn # Contains all the functions we need to to train our network\n",
    "import torch.nn.functional as F # Contains some additional functions such as activations\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbd52dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positional_embeddings(sequence_length, d): #\"i\" in paper corresponds to j - i.e. along dimension of size d\n",
    "    result = torch.ones(sequence_length, d) #pos in paper refers to which token - i.e. varying from 1 to 50\n",
    "    for i in range(sequence_length):\n",
    "      for j in range(d):\n",
    "        if j%2==0:\n",
    "          result[i,j] = math.sin(i/10000**(j/d))\n",
    "        else:\n",
    "          result[i,j] = math.cos(i/10000**((j-1)/d))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3d9574",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Selective_Transformer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_d, out_d, n_heads, n_blocks, pos_dim, attends):\n",
    "        super(Selective_Transformer, self).__init__()\n",
    "        self.pos_dim = pos_dim\n",
    "        #self.class_token = nn.Parameter(torch.rand((input_dim)))\n",
    "        self.attends = attends\n",
    "        self.hidden_d = hidden_d+pos_dim\n",
    "        \n",
    "        self.linear = nn.Linear(input_dim+pos_dim, self.hidden_d)\n",
    "        \n",
    "        self.blocks = nn.ModuleList([MyViTBlock(self.hidden_d, n_heads, attends, pos_dim) for _ in range(n_blocks)])\n",
    "        \n",
    "        self.mlp = nn.Linear(self.hidden_d,out_d)#for CELoss\n",
    "        #nn.Sequential(nn.Linear(hidden_d,out_d), nn.Softmax(dim=-1))\n",
    "        \n",
    "    def forward(self, sentence):\n",
    "        #print(sentence.shape)\n",
    "        pos = torch.unsqueeze(get_positional_embeddings(sentence.shape[-2], pos_dim).to(device),0)\n",
    "        #out = torch.cat((torch.unsqueeze(self.class_token+token_pos,0),sentence[0]),1)\n",
    "        #print(sentence.shape)\n",
    "        out = sentence[0]\n",
    "        out = torch.cat(out,pos)\n",
    "        out = self.linear(out) #so input is now hidden dim shape, i.e.1,s+1, h_d\n",
    "        for block in self.blocks:\n",
    "            out = block(out)    \n",
    "        out = torch.mean(out,dim=1)\n",
    "        \n",
    "        return torch.unsqueeze(self.mlp(out),0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39129fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMSA(nn.Module):\n",
    "    \"\"\"MSA block\"\"\"\n",
    "    def __init__(self, d, n_heads=2, attends, pos_dim):#d is hidden dim\n",
    "        super(MyMSA, self).__init__()\n",
    "        self.val_dim = d\n",
    "        if attends == \"val\":\n",
    "            self.d = d-pos_dim\n",
    "        else:\n",
    "            self.d = pos_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.attends = attends\n",
    "        self.pos_dim = pos_dim\n",
    "\n",
    "        assert d % n_heads == 0, f\"Can't divide dimension {d} into {n_heads} heads\"\n",
    "\n",
    "        d_head = int(d / n_heads) #dim of each head\n",
    "        self.q_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n",
    "        self.k_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n",
    "        self.v_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n",
    "        \n",
    "        if self.attends == \"pos\":\n",
    "            self.v_mappings = nn.ModuleList([nn.Linear(val_dim/n_heads, val_dim/n_heads) for _ in range(self.n_heads)])\n",
    "        self.d_head = d_head\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, sequences):#TODO: values only based on val section\n",
    "        # Sequences has shape (N, seq_length, token_dim)\n",
    "        # We go into shape    (N, seq_length, n_heads, token_dim / n_heads)\n",
    "        # And come back to    (N, seq_length, item_dim)  (through concatenation)\n",
    "        if self.attends == \"val\" # easy case\n",
    "            result = []\n",
    "            for sequence in sequences:\n",
    "                seq_result = []\n",
    "                for head in range(self.n_heads):\n",
    "                    q_mapping = self.q_mappings[head]\n",
    "                    k_mapping = self.k_mappings[head]\n",
    "                    v_mapping = self.v_mappings[head]\n",
    "\n",
    "                    #print(f\"{head=}\")\n",
    "\n",
    "                    seq = sequence[:, head * self.d_head: (head + 1) * self.d_head] #interesting? so each attention head only looks at a subset of features\n",
    "                    #print(seq.shape)\n",
    "                    q, k, v = q_mapping(seq), k_mapping(seq), v_mapping(seq)\n",
    "\n",
    "                    attention = self.softmax(q @ k.T / math.sqrt(self.d)) # here we take dot product between q and k vectors\n",
    "                    seq_result.append(attention @ v) #and here we do a weighted sum over v vectors based on attentions\n",
    "                result.append(torch.hstack(seq_result))    \n",
    "        elif self.attends == \"pos\":\n",
    "\n",
    "            result = []\n",
    "            for sequence in sequences:\n",
    "                seq_result = []\n",
    "                for head in range(self.n_heads):\n",
    "                    q_mapping = self.q_mappings[head]\n",
    "                    k_mapping = self.k_mappings[head]\n",
    "                    v_mapping = self.v_mappings[head]\n",
    "\n",
    "                    #print(f\"{head=}\")\n",
    "                    #so q,k attend over pos, val still from embs\n",
    "                    seq = sequence[:, val_dim+ head * self.d_head: val_dim+(head + 1) * self.d_head] #interesting? so each attention head only looks at a subset of features\n",
    "                    seq2 = sequence[:, head * self.val_dim/self.n_heads: (head + 1) * self.val_dim/self.n_heads]\n",
    "                    #print(seq.shape)\n",
    "                    q, k, v = q_mapping(seq), k_mapping(seq), v_mapping(seq2)\n",
    "\n",
    "                    attention = self.softmax(q @ k.T / math.sqrt(self.d)) # here we take dot product between q and k vectors\n",
    "                    seq_result.append(attention @ v) #and here we do a weighted sum over v vectors based on attentions\n",
    "                result.append(torch.hstack(seq_result))\n",
    "        return torch.cat([torch.unsqueeze(r, dim=0) for r in result])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937ef11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyViTBlock(nn.Module):\n",
    "    def __init__(self, hidden_d, n_heads, mlp_ratio=4, attends, pos_dim):\n",
    "        super(MyViTBlock, self).__init__()\n",
    "        self.hidden_d = hidden_d\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(hidden_d)\n",
    "        self.mhsa = MyMSA(hidden_d, n_heads, attends)\n",
    "        self.norm2 = nn.LayerNorm(hidden_d)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_d-pos_dim, mlp_ratio * (hidden_d-pos_dim)),#i.e. mlp ratio tells us how much bigger mlp hidden is than previous hidden\n",
    "            nn.GELU(),\n",
    "            nn.Linear(mlp_ratio * (hidden_d-pos_dim), (hidden_d-pos_dim))\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x + torch.cat((self.mhsa(self.norm1(x))[:,:,0:(hidden_d-pos_dim)],x[:,:,hidden_d-pos_dim:]),2) #so we do residual on multi self attention\n",
    "        out = out + torch.cat((self.mlp(self.norm2(out)[:,:,0:(hidden_d-pos_dim)]),self.norm2(out)[:,:,(hidden_d-pos_dim):])) #then residual on mlp\n",
    "        #importantly, this MLP can't update pos info\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae11981f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
